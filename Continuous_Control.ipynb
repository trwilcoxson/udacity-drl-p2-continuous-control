{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Continuous Control — DDPG with 20 Parallel Agents\n\nTrain a double-jointed arm to track moving target locations using **Deep Deterministic Policy Gradients (DDPG)**. The 20-agent version shares a single replay buffer and network to accelerate learning.\n\n**Solve condition**: Average score ≥ 30 over 100 consecutive episodes."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1. Import Packages and Start the Environment"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unityagents import UnityEnvironment\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom collections import deque\n\n%matplotlib inline"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "env = UnityEnvironment(file_name='Reacher.app')\n\n# Get the default brain\nbrain_name = env.brain_names[0]\nbrain = env.brains[brain_name]"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 2. Examine the State and Action Spaces"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Reset the environment\nenv_info = env.reset(train_mode=True)[brain_name]\n\n# Number of agents\nnum_agents = len(env_info.agents)\nprint('Number of agents:', num_agents)\n\n# Size of each action\naction_size = brain.vector_action_space_size\nprint('Size of each action:', action_size)\n\n# Examine the state space\nstates = env_info.vector_observations\nstate_size = states.shape[1]\nprint('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\nprint('The state for the first agent looks like:', states[0])"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 3. Take Random Actions in the Environment"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "env_info = env.reset(train_mode=True)[brain_name]\nstates = env_info.vector_observations\nscores = np.zeros(num_agents)\nwhile True:\n    actions = np.random.randn(num_agents, action_size)\n    actions = np.clip(actions, -1, 1)\n    env_info = env.step(actions)[brain_name]\n    next_states = env_info.vector_observations\n    rewards = env_info.rewards\n    dones = env_info.local_done\n    scores += env_info.rewards\n    states = next_states\n    if np.any(dones):\n        break\nprint('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 4. Train the Agent with DDPG"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from ddpg_agent import Agent\n\nagent = Agent(state_size=state_size, action_size=action_size,\n              num_agents=num_agents, random_seed=42)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ddpg_train(n_episodes=300, max_t=1000, print_every=10):\n    \"\"\"Train DDPG agent in the 20-agent Reacher environment.\n    \n    Returns:\n        all_scores: list of average scores (across 20 agents) per episode\n    \"\"\"\n    all_scores = []\n    scores_window = deque(maxlen=100)\n    solved = False\n    \n    for i_episode in range(1, n_episodes + 1):\n        env_info = env.reset(train_mode=True)[brain_name]\n        states = env_info.vector_observations\n        agent.reset()\n        scores = np.zeros(num_agents)\n        \n        for t in range(max_t):\n            actions = agent.act(states)\n            env_info = env.step(actions)[brain_name]\n            next_states = env_info.vector_observations\n            rewards = env_info.rewards\n            dones = env_info.local_done\n            agent.step(states, actions, rewards, next_states, dones, t)\n            states = next_states\n            scores += rewards\n            if np.any(dones):\n                break\n        \n        avg_score = np.mean(scores)\n        all_scores.append(avg_score)\n        scores_window.append(avg_score)\n        rolling_avg = np.mean(scores_window)\n        \n        print('\\rEpisode {}\\tAverage: {:.2f}\\tScore: {:.2f}'.format(\n            i_episode, rolling_avg, avg_score), end='')\n        \n        if i_episode % print_every == 0:\n            print('\\rEpisode {}\\tAverage: {:.2f}'.format(\n                i_episode, rolling_avg))\n        \n        if rolling_avg >= 30.0 and not solved:\n            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(\n                i_episode - 100, rolling_avg))\n            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n            solved = True\n    \n    # Save final weights if we haven't solved yet (or for the final state)\n    if not solved:\n        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n    \n    return all_scores\n\nscores = ddpg_train()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5. Plot Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\n# Episode scores\nax.plot(np.arange(1, len(scores) + 1), scores, alpha=0.3, color='steelblue', label='Episode Score')\n\n# 100-episode rolling average\nif len(scores) >= 100:\n    rolling = [np.mean(scores[max(0, i-100):i]) for i in range(1, len(scores) + 1)]\n    ax.plot(np.arange(1, len(scores) + 1), rolling, color='darkblue', linewidth=2, label='100-Episode Average')\n\n# Solve threshold\nax.axhline(y=30, color='red', linestyle='--', alpha=0.7, label='Solve Threshold (30)')\n\nax.set_xlabel('Episode')\nax.set_ylabel('Average Score (20 Agents)')\nax.set_title('DDPG Training — Continuous Control (20 Agents)')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('scores_plot.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6. Test the Trained Agent\n\nLoad saved weights and run 100 greedy episodes (no exploration noise) to verify performance."
  },
  {
   "cell_type": "code",
   "source": "# Load trained weights\nfrom ddpg_agent import Agent as TestAgent\n\ntest_agent = TestAgent(state_size=state_size, action_size=action_size,\n                       num_agents=num_agents, random_seed=42)\ntest_agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth', weights_only=True))\n\n# Run 100 greedy test episodes\nn_test = 100\ntest_scores = []\n\nfor i in range(1, n_test + 1):\n    env_info = env.reset(train_mode=True)[brain_name]\n    states = env_info.vector_observations\n    episode_scores = np.zeros(num_agents)\n    \n    while True:\n        actions = test_agent.act(states, add_noise=False)  # no exploration noise\n        env_info = env.step(actions)[brain_name]\n        states = env_info.vector_observations\n        episode_scores += env_info.rewards\n        if np.any(env_info.local_done):\n            break\n    \n    avg = np.mean(episode_scores)\n    test_scores.append(avg)\n    print('\\rTest Episode {}/{}\\tScore: {:.2f}'.format(i, n_test, avg), end='')\n\nprint('\\n\\nGreedy Test Results ({} episodes):'.format(n_test))\nprint('  Average: {:.2f}'.format(np.mean(test_scores)))\nprint('  Std Dev: {:.2f}'.format(np.std(test_scores)))\nprint('  Min:     {:.2f}'.format(np.min(test_scores)))\nprint('  Max:     {:.2f}'.format(np.max(test_scores)))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "env.close()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}